{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Alexthia: My Custom LLM for Reasoning\n\n**What I'm Building:** I'm fine-tuning Qwen 2.5 7B (7 billion parameters) to create my own reasoning model as part of the Axiom Stack.\n\n**My Approach:** I'm using QLoRA to efficiently train on 10,000+ examples from the MATH, GSM8K, and ScienceQA datasets.\n\n**Hardware:** Optimized for Kaggle P100/T4 GPUs (16GB VRAM)\n\n**Training Time:** ~6-8 hours on P100, ~10-12 hours on T4\n\n---\n\n## Setup Checklist\n1.  Enable GPU: Settings (right sidebar) â†’ Accelerator â†’ GPU T4 x2\n2. Enable Internet: Settings â†’ Internet â†’ On\n3.  Click \"Run All\" or run cells sequentially",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Install Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install transformers==4.45.2 datasets accelerate peft bitsandbytes trl==0.8.6 psutil",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:20.334279Z",
     "iopub.execute_input": "2025-12-27T22:12:20.334503Z",
     "iopub.status.idle": "2025-12-27T22:12:23.875289Z",
     "shell.execute_reply.started": "2025-12-27T22:12:20.334480Z",
     "shell.execute_reply": "2025-12-27T22:12:23.874385Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!rm -rf /kaggle/working/unsloth_compiled_cache",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:23.876611Z",
     "iopub.execute_input": "2025-12-27T22:12:23.876898Z",
     "iopub.status.idle": "2025-12-27T22:12:23.993479Z",
     "shell.execute_reply.started": "2025-12-27T22:12:23.876868Z",
     "shell.execute_reply": "2025-12-27T22:12:23.992556Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Restart the kernel to ensure clean imports\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:23.995021Z",
     "iopub.execute_input": "2025-12-27T22:12:23.995713Z",
     "iopub.status.idle": "2025-12-27T22:12:24.002057Z",
     "shell.execute_reply.started": "2025-12-27T22:12:23.995665Z",
     "shell.execute_reply": "2025-12-27T22:12:24.001339Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Verify GPU",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!nvidia-smi",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:24.004118Z",
     "iopub.execute_input": "2025-12-27T22:12:24.004402Z",
     "iopub.status.idle": "2025-12-27T22:12:24.227042Z",
     "shell.execute_reply.started": "2025-12-27T22:12:24.004380Z",
     "shell.execute_reply": "2025-12-27T22:12:24.226362Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Load Qwen 2.5 7B with 4-bit Quantization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-7B\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\" Model and tokenizer loaded\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:24.228139Z",
     "iopub.execute_input": "2025-12-27T22:12:24.228410Z",
     "iopub.status.idle": "2025-12-27T22:12:48.432117Z",
     "shell.execute_reply.started": "2025-12-27T22:12:24.228376Z",
     "shell.execute_reply": "2025-12-27T22:12:48.431470Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Add LoRA Adapters",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# CRITICAL: Prepare model for k-bit training FIRST\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,  \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# Verify setup\nmodel.print_trainable_parameters()\n\n# CRITICAL: Ensure model is in training mode\nmodel.train()\n\nprint(\" Model ready on\", next(model.parameters()).device)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:48.433116Z",
     "iopub.execute_input": "2025-12-27T22:12:48.433667Z",
     "iopub.status.idle": "2025-12-27T22:12:49.133468Z",
     "shell.execute_reply.started": "2025-12-27T22:12:48.433609Z",
     "shell.execute_reply": "2025-12-27T22:12:49.132817Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load Dataset (MATH, GSM8K, SCIQ for Reasoning)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import load_dataset, concatenate_datasets\nprint(\" Loading datasets...\")\n# MATH-Hard (2,304 examples)\nmath_dataset = load_dataset(\"lighteval/MATH-Hard\", split=\"train\")\nmath_dataset = math_dataset.rename_column(\"solution\", \"answer\")\nmath_dataset = math_dataset.rename_column(\"problem\", \"question\")\nprint(f\"âœ“ MATH-Hard: {len(math_dataset)} examples\")\n# GSM8K (3,000 examples)\ngsm8k_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\").select(range(3000))\n# Already has 'question' and 'answer' columns\nprint(f\"âœ“ GSM8K: {len(gsm8k_dataset)} examples\")\n# SciQ (2,000 examples)\nsciq_dataset = load_dataset(\"allenai/sciq\", split=\"train\").select(range(2000))\n# Build question from support + question, answer is correct_answer\ndef format_sciq(example):\n    question = f\"{example['support']} {example['question']}\" if example['support'] else example['question']\n    return {\"question\": question, \"answer\": example['correct_answer']}\nsciq_dataset = sciq_dataset.map(format_sciq)\nprint(f\"âœ“ SciQ: {len(sciq_dataset)} examples\")\n# Keep only 'question' and 'answer' columns\nmath_dataset = math_dataset.select_columns(['question', 'answer'])\ngsm8k_dataset = gsm8k_dataset.select_columns(['question', 'answer'])\nsciq_dataset = sciq_dataset.select_columns(['question', 'answer'])\n# Combine all datasets\ncombined_dataset = concatenate_datasets([math_dataset, gsm8k_dataset, sciq_dataset])\nprint(f\"\\n Total: {len(combined_dataset)} examples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:49.134459Z",
     "iopub.execute_input": "2025-12-27T22:12:49.134787Z",
     "iopub.status.idle": "2025-12-27T22:12:51.778939Z",
     "shell.execute_reply.started": "2025-12-27T22:12:49.134747Z",
     "shell.execute_reply": "2025-12-27T22:12:51.778312Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Format Dataset for Chat Template",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## 6. Format Dataset for Chat Template\n\nprint(\" Formatting datasets for training...\")\n\nalpaca_prompt = \"\"\"Below is a math problem. Write a solution that appropriately solves the problem.\n### Problem:\n{}\n### Solution:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_and_tokenize(examples):\n    # Format as text\n    texts = []\n    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n        texts.append(alpaca_prompt.format(q, a) + EOS_TOKEN)\n    \n    # Tokenize WITHOUT padding (dynamic padding later)\n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=2048,\n        padding=False, \n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_dataset = combined_dataset.map(\n    format_and_tokenize,\n    batched=True,\n    remove_columns=combined_dataset.column_names,\n)\n\nprint(f\" Tokenized {len(tokenized_dataset)} examples\")\n\n# Check token length distribution\nlengths = [len(ex) for ex in tokenized_dataset[\"input_ids\"]]\nprint(f\" Token length stats:\")\nprint(f\"   Min: {min(lengths)}, Max: {max(lengths)}, Avg: {sum(lengths)//len(lengths)}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:51.779833Z",
     "iopub.execute_input": "2025-12-27T22:12:51.780335Z",
     "iopub.status.idle": "2025-12-27T22:12:52.806451Z",
     "shell.execute_reply.started": "2025-12-27T22:12:51.780307Z",
     "shell.execute_reply": "2025-12-27T22:12:52.805824Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Configure Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nimport torch\n\n# Training arguments - REMOVED gradient_checkpointing from args\ntraining_args = TrainingArguments(\n    output_dir=\"./outputs\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    warmup_ratio=0.03,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    remove_unused_columns=False,\n    seed=3407,\n    max_grad_norm=0.3,\n    report_to=\"none\",\n    # gradient_checkpointing=True,  # REMOVED - causes issues with PEFT\n)\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\" Training Configuration:\")\nprint(f\"   â€¢ Total examples: {len(tokenized_dataset):,}\")\nprint(f\"   â€¢ Batch size per device: 1\")\nprint(f\"   â€¢ Gradient accumulation: 8\")\nprint(f\"   â€¢ Effective batch size: 8\")\nprint(f\"   â€¢ Epochs: 3\")\nprint(f\"   â€¢ Total steps: {len(tokenized_dataset) // 8 * 3:,}\")\nprint(f\"   â€¢ Learning rate: 2e-4\")\nprint(f\"   â€¢ Memory optimizations: 4-bit + QLoRA\")\nprint(f\"{'='*60}\\n\")\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:52.807323Z",
     "iopub.execute_input": "2025-12-27T22:12:52.807540Z",
     "iopub.status.idle": "2025-12-27T22:12:56.252743Z",
     "shell.execute_reply.started": "2025-12-27T22:12:52.807506Z",
     "shell.execute_reply": "2025-12-27T22:12:56.251939Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Start Training \n\n**Expected time:**\n- P100: ~6-8 hours\n- T4: ~10-12 hours\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## 8. Start Training\n\nimport time\n\nprint(\"Starting training...\")\nprint(f\" Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"  Estimated time: 8-10 hours on T4\")\nprint(\"\\n\" + \"=\"*60)\n\n# Train the model\ntrainer_stats = trainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\" Training complete!\")\nprint(f\" End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\" Final loss: {trainer_stats.training_loss:.4f}\")\nprint(f\"  Total time: {trainer_stats.metrics['train_runtime']/3600:.2f} hours\")\nprint(\"=\"*60)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-27T22:12:56.253778Z",
     "iopub.execute_input": "2025-12-27T22:12:56.254411Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Test the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model.eval()  \n# Test problem\ntest_problem = \"\"\"Below is a math problem. Write a solution that appropriately solves the problem.\n\n### Problem:\nIf $x^2 + 2x - 15 = 0$, what are the possible values of $x$?\n\n### Solution:\n\"\"\"\n\ninputs = tokenizer(test_problem, return_tensors=\"pt\").to(\"cuda\")\n\nprint(\" Test Problem:\")\nprint(test_problem)\nprint(\"\\n Alexthia's Solution:\")\nprint(\"=\"*50)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.9,\n    use_cache=True,\n)\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response[len(test_problem):])",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Save the Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model.save_pretrained(\"alexthia-v0.5\")\ntokenizer.save_pretrained(\"alexthia-v0.5\")\nprint(\"âœ“ Model saved to alexthia-v0.5/\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 11. (Optional) Save Full Merged Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Merge LoRA with base model and save in 16-bit (warning: ~14GB)\n# Only run this if you need a standalone model\n\n# model.save_pretrained_merged(\n#     \"alexthia-qwen-7b-merged\",\n#     tokenizer,\n#     save_method=\"merged_16bit\",\n# )\n\n# print(\" Merged model saved!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Evaluation & Benchmarking",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load test set\ntest_dataset = load_dataset(\"lighteval/MATH\", split=\"test\").shuffle(seed=42).select(range(100))\n\nprint(\" Running evaluation on 100 test problems...\")\ncorrect = 0\ntotal = 0\n\nfor example in test_dataset:\n    problem = example[\"problem\"]\n    true_solution = example[\"solution\"]\n    \n    # Generate solution\n    prompt = alpaca_prompt.format(problem, \"\")\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1)\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Simple check: does it contain key numbers/terms?\n    # (For proper eval, you'd need a math answer parser)\n    total += 1\n    \n    if total % 10 == 0:\n        print(f\"Progress: {total}/100\")\n\nprint(f\"\\n Evaluation complete!\")\nprint(f\" Sample size: {total} problems\")\nprint(\"\\n For detailed accuracy, use a proper MATH benchmark evaluator\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Export for Download",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nimport shutil\nimport os\n\nprint(\" Packaging model for download...\")\n\n# Create clean directory\nmodel_dir = \"alexthia-v0.5\"\nzip_name = \"alexthia-v0.5\"\n\n# Zip the model\nshutil.make_archive(zip_name, 'zip', model_dir)\n\n# Get file size\nzip_size = os.path.getsize(f\"{zip_name}.zip\") / (1024 * 1024)  # MB\n\nprint(f\" Model packaged!\")\nprint(f\" File: {zip_name}.zip\")\nprint(f\" Size: {zip_size:.1f} MB\")\nprint(f\"\\n To download:\")\nprint(f\"   1. Click 'Output' tab (right sidebar)\")\nprint(f\"   2. Find '{zip_name}.zip'\")\nprint(f\"   3. Click download icon\")\nprint(f\"\\n Ready to deploy Alexthia v0.5!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ðŸŽ‰ \n\nI've successfully fine-tuned Qwen 2.5 7B on mathematical reasoning.\n\n### Next Steps:\n1. Download the model weights (LoRA adapters)\n2. Test on my own game theory problems\n3. Integrate with Flux/tenet verifier (Phase 2)\n\n\n###  Pitch:\n> \"I fine-tuned Qwen 7B (7 billion parameters) on mathematical reasoning tasks using QLoRA, demonstrating my ability to work with state-of-the-art LLMs. This serves as the foundation for Alexthia, which will integrate formal verification through my  languages to reduce hallucination rates.\"\n\n---\n\n**Built by:** Fawaz  \n**Project:** Alexthia (Axiom Stack)  \n**Date:** December 2025",
   "metadata": {}
  }
 ]
}